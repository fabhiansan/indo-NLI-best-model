2025-03-23 22:29:23,615 - root - INFO - Configuration: {'model': {'name': 'Sentence-BERT-Proper', 'pretrained_model_name': 'firqaaa/indo-sentence-bert-base', 'max_seq_length': 128, 'output_hidden_states': True, 'classifier_type': 'proper', 'hidden_dropout_prob': 0.1, 'classifier_dropout': 0.1, 'classifier_hidden_dim': 768, 'classifier_num_layers': 2}, 'training': {'batch_size': 128, 'learning_rate': '2e-5', 'num_epochs': 5, 'warmup_ratio': 0.1, 'weight_decay': 0.01, 'gradient_accumulation_steps': 1, 'seed': 42, 'save_steps': 500, 'eval_steps': 500, 'logging_steps': 100, 'disable_tqdm': False, 'fp16': True}, 'data': {'dataset_name': 'afaji/indonli', 'train_split': 'train', 'validation_split': 'validation', 'test_splits': ['test_lay', 'test_expert'], 'num_workers': 4}, 'output': {'output_dir': '/home/jupyter-23522029/indo-NLI-best-model/models/sentence-bert-proper', 'logging_dir': '/home/jupyter-23522029/indo-NLI-best-model/logs/sentence-bert-proper', 'report_dir': '/home/jupyter-23522029/indo-NLI-best-model/reports/sentence-bert-proper'}}
2025-03-23 22:29:23,699 - root - INFO - System information:
2025-03-23 22:29:23,699 - root - INFO - Python version: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]
2025-03-23 22:29:23,699 - root - INFO - PyTorch version: 2.6.0+cu124
2025-03-23 22:29:23,699 - root - INFO - CUDA available: Yes
2025-03-23 22:29:23,699 - root - INFO - CUDA version: 12.4
2025-03-23 22:29:23,743 - root - INFO - Number of GPUs: 1
2025-03-23 22:29:23,749 - root - INFO - Current GPU: 0
2025-03-23 22:29:23,749 - root - INFO - GPU name: NVIDIA RTX A5000
2025-03-23 22:29:23,750 - root - INFO - Using device: cuda
2025-03-23 22:29:23,750 - root - INFO - Creating model
2025-03-23 22:29:23,750 - src.models.model_factory - INFO - Creating model of type Sentence-BERT-Proper
2025-03-23 22:29:23,750 - src.models.sentence_bert_model - INFO - Loading Sentence-BERT model from firqaaa/indo-sentence-bert-base
2025-03-23 22:29:25,407 - src.models.sentence_bert_model - INFO - Loading tokenizer from: firqaaa/indo-sentence-bert-base
2025-03-23 22:29:25,728 - root - INFO - Loading datasets
2025-03-23 22:29:25,728 - src.data.dataset - INFO - Loading IndoNLI dataset (train split)...
2025-03-23 22:29:28,117 - src.data.dataset - INFO - Loaded 10330 examples from train split
2025-03-23 22:29:28,118 - src.data.dataset - INFO - Loading IndoNLI dataset (validation split)...
2025-03-23 22:29:29,487 - src.data.dataset - INFO - Loaded 2197 examples from validation split
2025-03-23 22:29:29,487 - root - INFO - Creating trainer
2025-03-23 22:29:29,487 - src.training.trainer - INFO - Using device: cuda
2025-03-23 22:29:29,494 - root - INFO - Starting training
2025-03-23 22:29:29,494 - src.training.trainer - INFO - Starting training
2025-03-23 22:29:29,497 - src.training.trainer - INFO - Epoch 1/5
2025-03-23 22:29:48,083 - src.training.trainer - INFO - Epoch 1 completed. Average loss: 1.0311
2025-03-23 22:29:48,084 - src.training.trainer - INFO - Running evaluation
2025-03-23 22:29:53,131 - src.training.trainer - INFO - Eval accuracy: 0.5630405097860719
2025-03-23 22:29:53,131 - src.training.trainer - INFO - Eval precision: 0.5745481112110304
2025-03-23 22:29:53,131 - src.training.trainer - INFO - Eval recall: 0.546829982946639
2025-03-23 22:29:53,131 - src.training.trainer - INFO - Eval f1: 0.5466035194356865
2025-03-23 22:29:53,131 - src.training.trainer - INFO - Eval precision_entailment: 0.5090439276485789
2025-03-23 22:29:53,131 - src.training.trainer - INFO - Eval recall_entailment: 0.7323420074349443
2025-03-23 22:29:53,132 - src.training.trainer - INFO - Eval f1_entailment: 0.600609756097561
2025-03-23 22:29:53,132 - src.training.trainer - INFO - Eval precision_neutral: 0.4319148936170213
2025-03-23 22:29:53,132 - src.training.trainer - INFO - Eval recall_neutral: 0.3166926677067083
2025-03-23 22:29:53,132 - src.training.trainer - INFO - Eval f1_neutral: 0.36543654365436545
2025-03-23 22:29:53,132 - src.training.trainer - INFO - Eval precision_contradiction: 0.7826855123674912
2025-03-23 22:29:53,132 - src.training.trainer - INFO - Eval recall_contradiction: 0.5914552736982643
2025-03-23 22:29:53,132 - src.training.trainer - INFO - Eval f1_contradiction: 0.6737642585551331
2025-03-23 22:29:54,267 - src.models.sentence_bert_model - INFO - Loading tokenizer from: firqaaa/indo-sentence-bert-base
2025-03-23 22:29:54,586 - src.models.sentence_bert_model - INFO - Tokenizer saved successfully to /home/jupyter-23522029/indo-NLI-best-model/models/sentence-bert-proper/epoch-1
2025-03-23 22:29:54,624 - src.training.trainer - INFO - Model saved to /home/jupyter-23522029/indo-NLI-best-model/models/sentence-bert-proper/epoch-1
2025-03-23 22:29:54,624 - src.training.trainer - INFO - Epoch 2/5
2025-03-23 22:29:59,356 - src.training.trainer - INFO - Step 100: loss = 0.8788
2025-03-23 22:30:12,800 - src.training.trainer - INFO - Epoch 2 completed. Average loss: 0.8467
2025-03-23 22:30:12,801 - src.training.trainer - INFO - Running evaluation
2025-03-23 22:30:17,733 - src.training.trainer - INFO - Eval accuracy: 0.6781975421028675
2025-03-23 22:30:17,734 - src.training.trainer - INFO - Eval precision: 0.6867021223962692
2025-03-23 22:30:17,734 - src.training.trainer - INFO - Eval recall: 0.6785530697619557
2025-03-23 22:30:17,734 - src.training.trainer - INFO - Eval f1: 0.6776602549161619
2025-03-23 22:30:17,734 - src.training.trainer - INFO - Eval precision_entailment: 0.6761229314420804
2025-03-23 22:30:17,734 - src.training.trainer - INFO - Eval recall_entailment: 0.7087980173482032
2025-03-23 22:30:17,734 - src.training.trainer - INFO - Eval f1_entailment: 0.6920750151240169
2025-03-23 22:30:17,734 - src.training.trainer - INFO - Eval precision_neutral: 0.5921052631578947
2025-03-23 22:30:17,734 - src.training.trainer - INFO - Eval recall_neutral: 0.7020280811232449
2025-03-23 22:30:17,734 - src.training.trainer - INFO - Eval f1_neutral: 0.6423982869379015
2025-03-23 22:30:17,734 - src.training.trainer - INFO - Eval precision_contradiction: 0.7918781725888325
2025-03-23 22:30:17,734 - src.training.trainer - INFO - Eval recall_contradiction: 0.6248331108144193
2025-03-23 22:30:17,734 - src.training.trainer - INFO - Eval f1_contradiction: 0.6985074626865672
2025-03-23 22:30:18,884 - src.models.sentence_bert_model - INFO - Loading tokenizer from: firqaaa/indo-sentence-bert-base
2025-03-23 22:30:19,211 - src.models.sentence_bert_model - INFO - Tokenizer saved successfully to /home/jupyter-23522029/indo-NLI-best-model/models/sentence-bert-proper/epoch-2
2025-03-23 22:30:19,228 - src.training.trainer - INFO - Model saved to /home/jupyter-23522029/indo-NLI-best-model/models/sentence-bert-proper/epoch-2
2025-03-23 22:30:19,228 - src.training.trainer - INFO - Epoch 3/5
2025-03-23 22:30:28,131 - src.training.trainer - INFO - Step 200: loss = 0.6988
2025-03-23 22:30:37,536 - src.training.trainer - INFO - Epoch 3 completed. Average loss: 0.6899
2025-03-23 22:30:37,536 - src.training.trainer - INFO - Running evaluation
2025-03-23 22:30:42,562 - src.training.trainer - INFO - Eval accuracy: 0.7023213472917615
2025-03-23 22:30:42,562 - src.training.trainer - INFO - Eval precision: 0.708463868404935
2025-03-23 22:30:42,562 - src.training.trainer - INFO - Eval recall: 0.6987323052033528
2025-03-23 22:30:42,562 - src.training.trainer - INFO - Eval f1: 0.6993940864332275
2025-03-23 22:30:42,562 - src.training.trainer - INFO - Eval precision_entailment: 0.6773504273504274
2025-03-23 22:30:42,562 - src.training.trainer - INFO - Eval recall_entailment: 0.7856257744733581
2025-03-23 22:30:42,563 - src.training.trainer - INFO - Eval f1_entailment: 0.7274813539873781
2025-03-23 22:30:42,563 - src.training.trainer - INFO - Eval precision_neutral: 0.65402124430956
2025-03-23 22:30:42,563 - src.training.trainer - INFO - Eval recall_neutral: 0.672386895475819
2025-03-23 22:30:42,563 - src.training.trainer - INFO - Eval f1_neutral: 0.6630769230769231
2025-03-23 22:30:42,563 - src.training.trainer - INFO - Eval precision_contradiction: 0.7940199335548173
2025-03-23 22:30:42,563 - src.training.trainer - INFO - Eval recall_contradiction: 0.6381842456608812
2025-03-23 22:30:42,563 - src.training.trainer - INFO - Eval f1_contradiction: 0.7076239822353811
2025-03-23 22:30:43,733 - src.models.sentence_bert_model - INFO - Loading tokenizer from: firqaaa/indo-sentence-bert-base
2025-03-23 22:30:44,104 - src.models.sentence_bert_model - INFO - Tokenizer saved successfully to /home/jupyter-23522029/indo-NLI-best-model/models/sentence-bert-proper/epoch-3
2025-03-23 22:30:44,120 - src.training.trainer - INFO - Model saved to /home/jupyter-23522029/indo-NLI-best-model/models/sentence-bert-proper/epoch-3
2025-03-23 22:30:44,120 - src.training.trainer - INFO - Epoch 4/5
2025-03-23 22:30:57,216 - src.training.trainer - INFO - Step 300: loss = 0.5767
2025-03-23 22:31:02,477 - src.training.trainer - INFO - Epoch 4 completed. Average loss: 0.5793
2025-03-23 22:31:02,477 - src.training.trainer - INFO - Running evaluation
2025-03-23 22:31:07,493 - src.training.trainer - INFO - Eval accuracy: 0.7114246700045517
2025-03-23 22:31:07,494 - src.training.trainer - INFO - Eval precision: 0.7170972114618421
2025-03-23 22:31:07,494 - src.training.trainer - INFO - Eval recall: 0.709559543268696
2025-03-23 22:31:07,494 - src.training.trainer - INFO - Eval f1: 0.7097324110551405
2025-03-23 22:31:07,494 - src.training.trainer - INFO - Eval precision_entailment: 0.6891592920353983
2025-03-23 22:31:07,494 - src.training.trainer - INFO - Eval recall_entailment: 0.7719950433705081
2025-03-23 22:31:07,494 - src.training.trainer - INFO - Eval f1_entailment: 0.72822910578609
2025-03-23 22:31:07,494 - src.training.trainer - INFO - Eval precision_neutral: 0.6608187134502924
2025-03-23 22:31:07,494 - src.training.trainer - INFO - Eval recall_neutral: 0.7051482059282371
2025-03-23 22:31:07,494 - src.training.trainer - INFO - Eval f1_neutral: 0.6822641509433962
2025-03-23 22:31:07,494 - src.training.trainer - INFO - Eval precision_contradiction: 0.8013136288998358
2025-03-23 22:31:07,494 - src.training.trainer - INFO - Eval recall_contradiction: 0.6515353805073432
2025-03-23 22:31:07,494 - src.training.trainer - INFO - Eval f1_contradiction: 0.7187039764359352
2025-03-23 22:31:08,666 - src.models.sentence_bert_model - INFO - Loading tokenizer from: firqaaa/indo-sentence-bert-base
2025-03-23 22:31:09,327 - src.models.sentence_bert_model - INFO - Tokenizer saved successfully to /home/jupyter-23522029/indo-NLI-best-model/models/sentence-bert-proper/epoch-4
2025-03-23 22:31:09,345 - src.training.trainer - INFO - Model saved to /home/jupyter-23522029/indo-NLI-best-model/models/sentence-bert-proper/epoch-4
2025-03-23 22:31:09,345 - src.training.trainer - INFO - Epoch 5/5
2025-03-23 22:31:26,586 - src.training.trainer - INFO - Step 400: loss = 0.5091
2025-03-23 22:31:27,699 - src.training.trainer - INFO - Epoch 5 completed. Average loss: 0.5113
2025-03-23 22:31:27,699 - src.training.trainer - INFO - Running evaluation
2025-03-23 22:31:32,959 - src.training.trainer - INFO - Eval accuracy: 0.70368684569868
2025-03-23 22:31:32,959 - src.training.trainer - INFO - Eval precision: 0.7044059996742464
2025-03-23 22:31:32,959 - src.training.trainer - INFO - Eval recall: 0.7011229359162571
2025-03-23 22:31:32,959 - src.training.trainer - INFO - Eval f1: 0.7017807611385263
2025-03-23 22:31:32,959 - src.training.trainer - INFO - Eval precision_entailment: 0.6908045977011494
2025-03-23 22:31:32,959 - src.training.trainer - INFO - Eval recall_entailment: 0.7447335811648079
2025-03-23 22:31:32,960 - src.training.trainer - INFO - Eval f1_entailment: 0.7167561121049493
2025-03-23 22:31:32,960 - src.training.trainer - INFO - Eval precision_neutral: 0.6620583717357911
2025-03-23 22:31:32,960 - src.training.trainer - INFO - Eval recall_neutral: 0.672386895475819
2025-03-23 22:31:32,960 - src.training.trainer - INFO - Eval f1_neutral: 0.6671826625386997
2025-03-23 22:31:32,960 - src.training.trainer - INFO - Eval precision_contradiction: 0.7603550295857988
2025-03-23 22:31:32,960 - src.training.trainer - INFO - Eval recall_contradiction: 0.6862483311081442
2025-03-23 22:31:32,960 - src.training.trainer - INFO - Eval f1_contradiction: 0.7214035087719298
2025-03-23 22:31:34,118 - src.models.sentence_bert_model - INFO - Loading tokenizer from: firqaaa/indo-sentence-bert-base
2025-03-23 22:31:34,465 - src.models.sentence_bert_model - INFO - Tokenizer saved successfully to /home/jupyter-23522029/indo-NLI-best-model/models/sentence-bert-proper/epoch-5
2025-03-23 22:31:34,484 - src.training.trainer - INFO - Model saved to /home/jupyter-23522029/indo-NLI-best-model/models/sentence-bert-proper/epoch-5
2025-03-23 22:31:35,594 - src.models.sentence_bert_model - INFO - Loading tokenizer from: firqaaa/indo-sentence-bert-base
2025-03-23 22:31:35,932 - src.models.sentence_bert_model - INFO - Tokenizer saved successfully to /home/jupyter-23522029/indo-NLI-best-model/models/sentence-bert-proper/final
2025-03-23 22:31:35,985 - src.training.trainer - INFO - Model saved to /home/jupyter-23522029/indo-NLI-best-model/models/sentence-bert-proper/final
2025-03-23 22:31:35,985 - src.training.trainer - INFO - Training completed!
2025-03-23 22:31:36,059 - root - INFO - Best evaluation metric: -inf
2025-03-23 22:31:36,060 - root - INFO - Training completed!
