2025-03-23 22:02:21,805 - INFO - Configuration: {'model': {'name': 'Sentence-BERT-Simple', 'pretrained_model_name': 'firqaaa/indo-sentence-bert-base', 'max_seq_length': 128, 'output_hidden_states': True, 'classifier_type': 'simple', 'hidden_dropout_prob': 0.1, 'classifier_dropout': 0.1}, 'training': {'batch_size': 128, 'learning_rate': '2e-5', 'num_epochs': 5, 'warmup_ratio': 0.1, 'weight_decay': 0.01, 'gradient_accumulation_steps': 1, 'seed': 42, 'save_steps': 500, 'eval_steps': 500, 'logging_steps': 100, 'disable_tqdm': False, 'fp16': True}, 'data': {'dataset_name': 'afaji/indonli', 'train_split': 'train', 'validation_split': 'validation', 'test_splits': ['test_lay', 'test_expert'], 'num_workers': 4}, 'output': {'output_dir': '/home/jupyter-23522029/indo-NLI-best-model/models/sentence-bert-simple', 'logging_dir': '/home/jupyter-23522029/indo-NLI-best-model/logs/sentence-bert-simple', 'report_dir': '/home/jupyter-23522029/indo-NLI-best-model/reports/sentence-bert-simple'}}
2025-03-23 22:02:21,890 - INFO - System information:
2025-03-23 22:02:21,890 - INFO - Python version: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]
2025-03-23 22:02:21,890 - INFO - PyTorch version: 2.6.0+cu124
2025-03-23 22:02:21,890 - INFO - CUDA available: Yes
2025-03-23 22:02:21,890 - INFO - CUDA version: 12.4
2025-03-23 22:02:21,933 - INFO - Number of GPUs: 1
2025-03-23 22:02:21,942 - INFO - Current GPU: 0
2025-03-23 22:02:21,942 - INFO - GPU name: NVIDIA RTX A5000
2025-03-23 22:02:21,943 - INFO - Using device: cuda
2025-03-23 22:02:21,943 - INFO - Creating model
2025-03-23 22:02:21,943 - INFO - Creating model of type Sentence-BERT-Simple
2025-03-23 22:02:21,944 - INFO - Loading Sentence-BERT model from firqaaa/indo-sentence-bert-base
2025-03-23 22:02:24,896 - INFO - Loading datasets
2025-03-23 22:02:24,896 - INFO - Loading IndoNLI dataset (train split)...
2025-03-23 22:02:27,325 - INFO - Loaded 10330 examples from train split
2025-03-23 22:02:27,327 - INFO - Loading IndoNLI dataset (validation split)...
2025-03-23 22:02:28,680 - INFO - Loaded 2197 examples from validation split
2025-03-23 22:02:28,681 - INFO - Creating trainer
2025-03-23 22:02:28,681 - INFO - Using device: cuda
2025-03-23 22:02:28,689 - INFO - Starting training
2025-03-23 22:02:28,689 - INFO - Starting training
/home/jupyter-23522029/indo-NLI-best-model/src/training/trainer.py:122: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
2025-03-23 22:02:28,691 - INFO - Epoch 1/5
  0%|                                         | 0/81 [00:00<?, ?it/s]/home/jupyter-23522029/indo-NLI-best-model/src/training/trainer.py:139: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Loss: 1.1060:   0%|                           | 0/81 [00:01<?, ?it/s]Loss: 1.1060:   1%|▏                  | 1/81 [00:01<01:52,  1.40s/it]Loss: 1.1123:   1%|▏                  | 1/81 [00:01<01:52,  1.40s/it]Loss: 1.1123:   2%|▍                  | 2/81 [00:01<00:55,  1.43it/s]Loss: 1.1093:   2%|▍                  | 2/81 [00:01<00:55,  1.43it/s]Loss: 1.1093:   4%|▋                  | 3/81 [00:01<00:37,  2.09it/s]Loss: 1.0895:   4%|▋                  | 3/81 [00:02<00:37,  2.09it/s]Loss: 1.0895:   5%|▉                  | 4/81 [00:02<00:28,  2.68it/s]Loss: 1.0921:   5%|▉                  | 4/81 [00:02<00:28,  2.68it/s]Loss: 1.0921:   6%|█▏                 | 5/81 [00:02<00:24,  3.15it/s]Loss: 1.0960:   6%|█▏                 | 5/81 [00:02<00:24,  3.15it/s]Loss: 1.0960:   7%|█▍                 | 6/81 [00:02<00:21,  3.54it/s]Loss: 1.1007:   7%|█▍                 | 6/81 [00:02<00:21,  3.54it/s]Loss: 1.1007:   9%|█▋                 | 7/81 [00:02<00:19,  3.86it/s]Loss: 1.0709:   9%|█▋                 | 7/81 [00:02<00:19,  3.86it/s]Loss: 1.0709:  10%|█▉                 | 8/81 [00:02<00:17,  4.10it/s]Loss: 1.1240:  10%|█▉                 | 8/81 [00:03<00:17,  4.10it/s]Loss: 1.1240:  11%|██                 | 9/81 [00:03<00:16,  4.26it/s]Loss: 1.1171:  11%|██                 | 9/81 [00:03<00:16,  4.26it/s]Loss: 1.1171:  12%|██▏               | 10/81 [00:03<00:16,  4.37it/s]Loss: 1.1175:  12%|██▏               | 10/81 [00:03<00:16,  4.37it/s]Loss: 1.1175:  14%|██▍               | 11/81 [00:03<00:15,  4.46it/s]Loss: 1.1006:  14%|██▍               | 11/81 [00:03<00:15,  4.46it/s]Loss: 1.1006:  15%|██▋               | 12/81 [00:03<00:15,  4.51it/s]Loss: 1.1007:  15%|██▋               | 12/81 [00:03<00:15,  4.51it/s]Loss: 1.1007:  16%|██▉               | 13/81 [00:03<00:14,  4.55it/s]Loss: 1.1361:  16%|██▉               | 13/81 [00:04<00:14,  4.55it/s]Loss: 1.1361:  17%|███               | 14/81 [00:04<00:14,  4.59it/s]Loss: 1.0820:  17%|███               | 14/81 [00:04<00:14,  4.59it/s]Loss: 1.0820:  19%|███▎              | 15/81 [00:04<00:14,  4.62it/s]Loss: 1.1035:  19%|███▎              | 15/81 [00:04<00:14,  4.62it/s]Loss: 1.1035:  20%|███▌              | 16/81 [00:04<00:14,  4.64it/s]Loss: 1.0859:  20%|███▌              | 16/81 [00:04<00:14,  4.64it/s]Loss: 1.0859:  21%|███▊              | 17/81 [00:04<00:13,  4.65it/s]Loss: 1.0773:  21%|███▊              | 17/81 [00:05<00:13,  4.65it/s]Loss: 1.0773:  22%|████              | 18/81 [00:05<00:13,  4.66it/s]Loss: 1.1005:  22%|████              | 18/81 [00:05<00:13,  4.66it/s]Loss: 1.1005:  23%|████▏             | 19/81 [00:05<00:13,  4.66it/s]Loss: 1.1075:  23%|████▏             | 19/81 [00:05<00:13,  4.66it/s]Loss: 1.1075:  25%|████▍             | 20/81 [00:05<00:13,  4.67it/s]Loss: 1.0747:  25%|████▍             | 20/81 [00:05<00:13,  4.67it/s]Loss: 1.0747:  26%|████▋             | 21/81 [00:05<00:12,  4.67it/s]Loss: 1.0776:  26%|████▋             | 21/81 [00:05<00:12,  4.67it/s]Loss: 1.0776:  27%|████▉             | 22/81 [00:05<00:12,  4.66it/s]Loss: 1.0988:  27%|████▉             | 22/81 [00:06<00:12,  4.66it/s]Loss: 1.0988:  28%|█████             | 23/81 [00:06<00:12,  4.67it/s]Loss: 1.0834:  28%|█████             | 23/81 [00:06<00:12,  4.67it/s]Loss: 1.0834:  30%|█████▎            | 24/81 [00:06<00:12,  4.66it/s]Loss: 1.0788:  30%|█████▎            | 24/81 [00:06<00:12,  4.66it/s]Loss: 1.0788:  31%|█████▌            | 25/81 [00:06<00:12,  4.67it/s]Loss: 1.0784:  31%|█████▌            | 25/81 [00:06<00:12,  4.67it/s]Loss: 1.0784:  32%|█████▊            | 26/81 [00:06<00:11,  4.67it/s]Loss: 1.0774:  32%|█████▊            | 26/81 [00:06<00:11,  4.67it/s]Loss: 1.0774:  33%|██████            | 27/81 [00:06<00:11,  4.66it/s]Loss: 1.0645:  33%|██████            | 27/81 [00:07<00:11,  4.66it/s]Loss: 1.0645:  35%|██████▏           | 28/81 [00:07<00:11,  4.67it/s]Loss: 1.0519:  35%|██████▏           | 28/81 [00:07<00:11,  4.67it/s]Loss: 1.0519:  36%|██████▍           | 29/81 [00:07<00:11,  4.67it/s]Loss: 1.0401:  36%|██████▍           | 29/81 [00:07<00:11,  4.67it/s]Loss: 1.0401:  37%|██████▋           | 30/81 [00:07<00:10,  4.67it/s]Loss: 1.0629:  37%|██████▋           | 30/81 [00:07<00:10,  4.67it/s]Loss: 1.0629:  38%|██████▉           | 31/81 [00:07<00:10,  4.67it/s]Loss: 1.0618:  38%|██████▉           | 31/81 [00:08<00:10,  4.67it/s]Loss: 1.0618:  40%|███████           | 32/81 [00:08<00:10,  4.66it/s]Loss: 1.0345:  40%|███████           | 32/81 [00:08<00:10,  4.66it/s]Loss: 1.0345:  41%|███████▎          | 33/81 [00:08<00:10,  4.66it/s]Loss: 1.0057:  41%|███████▎          | 33/81 [00:08<00:10,  4.66it/s]Loss: 1.0057:  42%|███████▌          | 34/81 [00:08<00:10,  4.67it/s]Loss: 1.0303:  42%|███████▌          | 34/81 [00:08<00:10,  4.67it/s]Loss: 1.0303:  43%|███████▊          | 35/81 [00:08<00:09,  4.67it/s]Loss: 1.0406:  43%|███████▊          | 35/81 [00:08<00:09,  4.67it/s]Loss: 1.0406:  44%|████████          | 36/81 [00:08<00:09,  4.67it/s]Loss: 0.9596:  44%|████████          | 36/81 [00:09<00:09,  4.67it/s]Loss: 0.9596:  46%|████████▏         | 37/81 [00:09<00:09,  4.64it/s]Loss: 1.0237:  46%|████████▏         | 37/81 [00:09<00:09,  4.64it/s]Loss: 1.0237:  47%|████████▍         | 38/81 [00:09<00:09,  4.62it/s]Loss: 1.0295:  47%|████████▍         | 38/81 [00:09<00:09,  4.62it/s]Loss: 1.0295:  48%|████████▋         | 39/81 [00:09<00:09,  4.63it/s]Loss: 0.9988:  48%|████████▋         | 39/81 [00:09<00:09,  4.63it/s]Loss: 0.9988:  49%|████████▉         | 40/81 [00:09<00:08,  4.63it/s]Loss: 0.9172:  49%|████████▉         | 40/81 [00:09<00:08,  4.63it/s]Loss: 0.9172:  51%|█████████         | 41/81 [00:09<00:08,  4.64it/s]Loss: 0.9179:  51%|█████████         | 41/81 [00:10<00:08,  4.64it/s]Loss: 0.9179:  52%|█████████▎        | 42/81 [00:10<00:08,  4.64it/s]Loss: 0.9549:  52%|█████████▎        | 42/81 [00:10<00:08,  4.64it/s]Loss: 0.9549:  53%|█████████▌        | 43/81 [00:10<00:08,  4.66it/s]Loss: 0.9214:  53%|█████████▌        | 43/81 [00:10<00:08,  4.66it/s]Loss: 0.9214:  54%|█████████▊        | 44/81 [00:10<00:07,  4.66it/s]Loss: 0.9587:  54%|█████████▊        | 44/81 [00:10<00:07,  4.66it/s]Loss: 0.9587:  56%|██████████        | 45/81 [00:10<00:07,  4.66it/s]Loss: 1.0167:  56%|██████████        | 45/81 [00:11<00:07,  4.66it/s]Loss: 1.0167:  57%|██████████▏       | 46/81 [00:11<00:07,  4.64it/s]Loss: 0.9613:  57%|██████████▏       | 46/81 [00:11<00:07,  4.64it/s]Loss: 0.9613:  58%|██████████▍       | 47/81 [00:11<00:07,  4.66it/s]Loss: 1.0603:  58%|██████████▍       | 47/81 [00:11<00:07,  4.66it/s]Loss: 1.0603:  59%|██████████▋       | 48/81 [00:11<00:07,  4.66it/s]Loss: 0.9095:  59%|██████████▋       | 48/81 [00:11<00:07,  4.66it/s]Loss: 0.9095:  60%|██████████▉       | 49/81 [00:11<00:06,  4.67it/s]Loss: 0.9684:  60%|██████████▉       | 49/81 [00:11<00:06,  4.67it/s]Loss: 0.9684:  62%|███████████       | 50/81 [00:11<00:06,  4.67it/s]Loss: 0.9900:  62%|███████████       | 50/81 [00:12<00:06,  4.67it/s]Loss: 0.9900:  63%|███████████▎      | 51/81 [00:12<00:06,  4.65it/s]Loss: 0.8715:  63%|███████████▎      | 51/81 [00:12<00:06,  4.65it/s]Loss: 0.8715:  64%|███████████▌      | 52/81 [00:12<00:06,  4.64it/s]Loss: 0.9657:  64%|███████████▌      | 52/81 [00:12<00:06,  4.64it/s]Loss: 0.9657:  65%|███████████▊      | 53/81 [00:12<00:06,  4.65it/s]Loss: 0.9596:  65%|███████████▊      | 53/81 [00:12<00:06,  4.65it/s]Loss: 0.9596:  67%|████████████      | 54/81 [00:12<00:05,  4.65it/s]Loss: 1.0073:  67%|████████████      | 54/81 [00:12<00:05,  4.65it/s]Loss: 1.0073:  68%|████████████▏     | 55/81 [00:12<00:05,  4.66it/s]Loss: 0.9045:  68%|████████████▏     | 55/81 [00:13<00:05,  4.66it/s]Loss: 0.9045:  69%|████████████▍     | 56/81 [00:13<00:05,  4.66it/s]Loss: 0.9306:  69%|████████████▍     | 56/81 [00:13<00:05,  4.66it/s]Loss: 0.9306:  70%|████████████▋     | 57/81 [00:13<00:05,  4.66it/s]Loss: 0.9230:  70%|████████████▋     | 57/81 [00:13<00:05,  4.66it/s]Loss: 0.9230:  72%|████████████▉     | 58/81 [00:13<00:04,  4.66it/s]Loss: 0.9583:  72%|████████████▉     | 58/81 [00:13<00:04,  4.66it/s]Loss: 0.9583:  73%|█████████████     | 59/81 [00:13<00:04,  4.66it/s]Loss: 0.9992:  73%|█████████████     | 59/81 [00:14<00:04,  4.66it/s]Loss: 0.9992:  74%|█████████████▎    | 60/81 [00:14<00:04,  4.65it/s]Loss: 0.9291:  74%|█████████████▎    | 60/81 [00:14<00:04,  4.65it/s]Loss: 0.9291:  75%|█████████████▌    | 61/81 [00:14<00:04,  4.66it/s]Loss: 0.9697:  75%|█████████████▌    | 61/81 [00:14<00:04,  4.66it/s]Loss: 0.9697:  77%|█████████████▊    | 62/81 [00:14<00:04,  4.66it/s]Loss: 0.9018:  77%|█████████████▊    | 62/81 [00:14<00:04,  4.66it/s]Loss: 0.9018:  78%|██████████████    | 63/81 [00:14<00:03,  4.65it/s]Loss: 0.9519:  78%|██████████████    | 63/81 [00:14<00:03,  4.65it/s]Loss: 0.9519:  79%|██████████████▏   | 64/81 [00:14<00:03,  4.62it/s]Loss: 0.9527:  79%|██████████████▏   | 64/81 [00:15<00:03,  4.62it/s]Loss: 0.9527:  80%|██████████████▍   | 65/81 [00:15<00:03,  4.63it/s]Loss: 0.9145:  80%|██████████████▍   | 65/81 [00:15<00:03,  4.63it/s]Loss: 0.9145:  81%|██████████████▋   | 66/81 [00:15<00:03,  4.64it/s]